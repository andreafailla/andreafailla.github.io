---
title: Generative AI and Large Language Models
summary: Introductory lecture on Generative AI and Large Language Models
date: 2025-09-11
type: docs
math: false
tags:
  - Generative AI
  - LLM
  - Prompt Engineering
image:
  caption:
---

This is the page for the **Generative AI and Large Language Models** lecture, an introductory course covering the foundations, practical usage, and safety aspects of modern LLM-based systems.
This course was offered as part of [SoBigData's Challenge Us](https://sobigdata.it/challenge-us-2025/it) program.

### Course Description
This course provides a high-level yet comprehensive overview of **Generative Artificial Intelligence**, with a particular focus on **Large Language Models (LLMs)**.  
The lecture combines conceptual foundations with practical guidance on how to effectively and responsibly use LLMs in real-world scenarios.

Topics covered include:
- Foundations of Generative AI and LLMs
- How LLMs work (training, transformers, scaling laws)
- Controlling and evaluating model outputs
- Prompt engineering techniques
- Advanced concepts: agents, memory, and Retrieval-Augmented Generation (RAG)
- Multimodal LLMs
- Safety, security, and misuse risks

---

### Lecture 1: Generative AI and Large Language Models

**Synopsis:**  
This lecture introduces Generative AI and Large Language Models, explaining what they are, how they are trained, and why they exhibit emergent abilities. We explore practical aspects such as parameter tuning and prompt engineering, discuss advanced system designs (agents, memory, RAG), and conclude with an overview of multimodal models and key safety challenges, including jailbreaking and prompt injection.

**Slides:**  
[here](https://docs.google.com/presentation/d/15CqoXQARhiGiSBf_X0ZpsHuqTBv9YqjBsHRcPzSgDuc/edit?usp=sharing)

---

### Main Topics

**Foundations**
- What is Generative AI
- Large Language Models: pretraining and fine-tuning
- Transformers and self-attention
- Model scaling and emergent abilities

**Using LLMs Effectively**
- Decoding parameters (temperature, top-k, top-p)
- Choosing parameters for different tasks
- Principles of prompt engineering
- Zero-shot, one-shot, and few-shot prompting
- Advanced prompting techniques (Chain-of-Thought, Tree-of-Thought, self-consistency)

**Advanced Architectures**
- Output verification and validation
- Model quantization
- Memory strategies for LLM-based systems
- Agents and tool-augmented reasoning
- Retrieval-Augmented Generation (RAG) and its variants
- Multimodal LLMs (text + images)

**Safety and Security**
- Hallucinations and factual reliability
- Biases and alignment issues
- Jailbreaking and prompt injection
- Data poisoning and misuse risks

---

### Suggested Readings (freely available)
- Bubeck et al. (2023). *Sparks of Artificial General Intelligence: Early experiments with GPT-4.*
- Wei et al. (2022). *Emergent abilities of large language models.*
- Vaswani et al. (2017). *Attention Is All You Need.*
- Zhao et al. (2023). *A Survey of Large Language Models.*
- Alammar & Grootendorst (2024). *Hands-On Large Language Models.*

---

#### Feedback
I am continuously looking for ways to improve my teaching and course material.  
If you attended this lecture, I would greatly appreciate any feedback you are willing to share.
